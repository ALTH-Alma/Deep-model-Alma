{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c75e0a74-54f4-46b1-8f5a-59e32d31fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# establecer semillas\n",
    "seed = 4734\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8348bf73-61ae-4167-b9b5-ff3c23206874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de GPUs disponibles: 4\n",
      "Detalles de GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Detalles de GPU 1: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "Detalles de GPU 2: PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')\n",
      "Detalles de GPU 3: PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 07:37:26.919276: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-07-03 07:37:26.921101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2024-07-03 07:37:26.961754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: Tesla P100-SXM2-16GB computeCapability: 6.0\n",
      "coreClock: 1.4805GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2024-07-03 07:37:26.962104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:06:00.0 name: Tesla P100-SXM2-16GB computeCapability: 6.0\n",
      "coreClock: 1.4805GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2024-07-03 07:37:26.962438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:84:00.0 name: Tesla P100-SXM2-16GB computeCapability: 6.0\n",
      "coreClock: 1.4805GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2024-07-03 07:37:26.962767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: \n",
      "pciBusID: 0000:85:00.0 name: Tesla P100-SXM2-16GB computeCapability: 6.0\n",
      "coreClock: 1.4805GHz coreCount: 56 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2024-07-03 07:37:26.962791: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-07-03 07:37:26.965732: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2024-07-03 07:37:26.965789: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2024-07-03 07:37:26.968273: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-07-03 07:37:26.968711: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-07-03 07:37:26.971701: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-07-03 07:37:26.973068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-07-03 07:37:26.978257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-07-03 07:37:26.980497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\n"
     ]
    }
   ],
   "source": [
    "# Verificar que TensorFlow detecta las GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configurar TensorFlow para permitir el crecimiento de la memoria GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"GPUs Not Available\")\n",
    "\n",
    "\n",
    "if gpus:\n",
    "    print(f'Número de GPUs disponibles: {len(gpus)}')\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f'Detalles de GPU {i}: {gpu}')\n",
    "else:\n",
    "    print('No se detectaron GPUs, se utilizará la CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21efb0-f152-41e9-ac54-31c20d4c49ee",
   "metadata": {},
   "source": [
    "### DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf76a870-6f8c-43c0-ab5a-920d9fe6d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos numpy normalizados - imagenes y velocidades\n",
    "\n",
    "# Rutas de trabajo\n",
    "npy_folder = '/disk2/alma/tesisSpace/'\n",
    "numpy_file_name_image = \"numpyFile_simulation_images\"\n",
    "numpy_file_name_velocity_x = \"./numpyFile_simulation_velocity_x\"\n",
    "numpy_file_name_velocity_y = \"./numpyFile_simulation_velocity_y\"\n",
    "\n",
    "def load_normalize_data():\n",
    "    \n",
    "    # Mensaje de depuración\n",
    "    print(\"Ready to load NUMPY images_normalize from \" + npy_folder)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Carga del archivo de imagenes\n",
    "    try:\n",
    "        data_norm_img = np.load(npy_folder + numpy_file_name_image + \".npy\", mmap_mode=None)\n",
    "    except:\n",
    "        print(\"ERROR loading file: \" + npy_folder + numpy_file_name_image + \".npy\")\n",
    "        sys.stdout.flush()\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(\"The file \" + npy_folder + numpy_file_name_image + \".npy was loaded.\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Carga del archivo de velocidades por el momento se toman velocidades verticales y horizontales por separado (Velocidades del gas)\n",
    "\n",
    "    try:\n",
    "        data_norm_vx = np.load(npy_folder + numpy_file_name_velocity_x + \".npy\", mmap_mode=None)\n",
    "    except:\n",
    "        print(\"ERROR loading file: \" + npy_folder + numpy_file_name_velocity_x + \".npy\")\n",
    "        sys.stdout.flush()\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(\"The file \" + npy_folder + numpy_file_name_velocity_x + \".npy was loaded.\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    try:\n",
    "        data_norm_vy = np.load(npy_folder + numpy_file_name_velocity_y + \".npy\", mmap_mode=None)\n",
    "    except:\n",
    "        print(\"ERROR loading file: \" + npy_folder + numpy_file_name_velocity_y + \".npy\")\n",
    "        sys.stdout.flush()\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(\"The file \" + npy_folder + numpy_file_name_velocity_y + \".npy was loaded.\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    \n",
    "    # Retorno de las matrices numpy\n",
    "    return data_norm_img, data_norm_vx, data_norm_vy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd13965b-7da9-4057-9c5a-33e320970b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adapt_training_data_Keras(data_img_norm, data_vel_norm):\n",
    "    print(\"\\n ADAPTING DATA FOR KERAS___________________________\")\n",
    "    \n",
    "    # Imprimir detalles iniciales del array de datos\n",
    "    print(\"I1 dtype =\", data_img_norm.dtype, \"- shape =\", data_img_norm.shape)\n",
    "    print(\"V1 dtype =\", data_vel_norm.dtype, \"- shape =\", data_vel_norm.shape)\n",
    "    \n",
    "    # Expandir el array de imágenes para incluir el canal como una dimensión adicional\n",
    "    # (batch_size, time_step, pix, pix) ---> (batch_size, time_step, pix, pix, 1)\n",
    "    data_img_norm = np.expand_dims(data_img_norm, axis=-1)\n",
    "    print(\"I2 dtype =\", data_img_norm.dtype, \"- shape =\", data_img_norm.shape)\n",
    "\n",
    "    # Expandir el array de velocidades para incluir el canal como una dimensión adicional\n",
    "    # (batch_size, pix, pix) ---> (batch_size, pix, pix, 1)\n",
    "    data_vel_norm = np.expand_dims(data_vel_norm, axis=-1)\n",
    "    print(\"V2 dtype =\", data_vel_norm.dtype, \"- shape =\", data_vel_norm.shape)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Inicialización de listas de índices\n",
    "    random_list_indexes_train = []\n",
    "    random_list_indexes_valid = []\n",
    "    random_list_indexes_test = []\n",
    "    \n",
    "    total_data_len = data_img_norm.shape[0]\n",
    "\n",
    "    # Crear índices para dividir los datos\n",
    "    indices = np.arange(total_data_len)\n",
    "    \n",
    "    # Dividir los datos en conjuntos de entrenamiento + validación y prueba\n",
    "    random_indexes_valid, random_indexes_test = train_test_split(\n",
    "        indices, test_size=0.2, random_state=23)\n",
    "    \n",
    "    # Dividir los datos de entrenamiento + validación en entrenamiento y validación\n",
    "    random_indexes_train, random_indexes_valid = train_test_split(\n",
    "        random_indexes_valid, test_size=0.25, random_state=23)\n",
    "    \n",
    "    # Selección de imágenes para los conjuntos de entrenamiento, validación y prueba\n",
    "    train_images = data_img_norm[random_indexes_train]\n",
    "    valid_images = data_img_norm[random_indexes_valid]\n",
    "    test_images = data_img_norm[random_indexes_test]\n",
    "    print(\"Train_images dtype =\", train_images.dtype, \"- shape =\", train_images.shape)\n",
    "    print(\"Valid_images =\", valid_images.dtype, \"- shape =\", valid_images.shape)\n",
    "    print(\"Test_images =\", test_images.dtype, \"- shape =\", test_images.shape)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    train_velocitys = data_vel_norm[random_indexes_train]\n",
    "    valid_velocitys = data_vel_norm[random_indexes_valid]\n",
    "    test_velocitys = data_vel_norm[random_indexes_test]\n",
    "    print(\"Train_velocity dtype =\", train_velocitys.dtype, \"- shape =\", train_velocitys.shape)\n",
    "    print(\"Valid_velocity dtype =\", valid_velocitys.dtype, \"- shape =\", valid_velocitys.shape)\n",
    "    print(\"Test_velocity dtype =\", test_velocitys.dtype, \"- shape =\", test_velocitys.shape)\n",
    "    \n",
    "    # No hay etiquetas, así que solo se devuelven las imágenes y velocidades\n",
    "    return train_images, valid_images, test_images, train_velocitys, valid_velocitys, test_velocitys, total_data_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65adf09b-7039-4faa-a215-525ba4b5f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dataset para el entrenamiento\n",
    "\n",
    "def prepare_datasets(train_images, valid_images, test_images, train_velocity, valid_velocity, test_velocity, batch_size):\n",
    "    print(\"\\n READY TO PREPARE DATASETS______________________\")\n",
    "    \n",
    "    # Convertir a tensores de TensorFlow\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_velocity)).batch(batch_size).shuffle(buffer_size=len(train_images))\n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images, valid_velocity)).batch(batch_size)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_velocity)).batch(batch_size)\n",
    "    \n",
    "    print(\"Train_dataset = \", train_dataset)\n",
    "    print(\"Valid_dataset = \", valid_dataset)\n",
    "    print(\"Test_dataset = \", test_dataset)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return train_dataset, valid_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c02a5f40-1a01-4b19-ab16-aac7b4f8d828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to load NUMPY images_normalize from /disk2/alma/tesisSpace/\n",
      "The file /disk2/alma/tesisSpace/numpyFile_simulation_images.npy was loaded.\n",
      "The file /disk2/alma/tesisSpace/./numpyFile_simulation_velocity_x.npy was loaded.\n",
      "The file /disk2/alma/tesisSpace/./numpyFile_simulation_velocity_y.npy was loaded.\n",
      "Image Shape (77, 19, 60, 60)\n",
      "Image Max:  0.5261954  Image Min:  0.0\n",
      "Vx Shape (77, 300, 300)\n",
      "Vx Max:  1.0  Vx Min:  0.0\n",
      "Vy Shape (77, 60, 60)\n",
      "Vy Max:  1.0  Vy Min:  0.0\n",
      "\n",
      " ADAPTING DATA FOR KERAS___________________________\n",
      "I1 dtype = float32 - shape = (77, 19, 60, 60)\n",
      "V1 dtype = float32 - shape = (77, 60, 60)\n",
      "I2 dtype = float32 - shape = (77, 19, 60, 60, 1)\n",
      "V2 dtype = float32 - shape = (77, 60, 60, 1)\n",
      "\n",
      "\n",
      "Train_images dtype = float32 - shape = (45, 19, 60, 60, 1)\n",
      "Valid_images = float32 - shape = (16, 19, 60, 60, 1)\n",
      "Test_images = float32 - shape = (16, 19, 60, 60, 1)\n",
      "\n",
      "\n",
      "Train_velocity dtype = float32 - shape = (45, 60, 60, 1)\n",
      "Valid_velocity dtype = float32 - shape = (16, 60, 60, 1)\n",
      "Test_velocity dtype = float32 - shape = (16, 60, 60, 1)\n",
      "\n",
      " READY TO PREPARE DATASETS______________________\n",
      "Train_dataset =  <ShuffleDataset shapes: ((None, 19, 60, 60, 1), (None, 60, 60, 1)), types: (tf.float32, tf.float32)>\n",
      "Valid_dataset =  <BatchDataset shapes: ((None, 19, 60, 60, 1), (None, 60, 60, 1)), types: (tf.float32, tf.float32)>\n",
      "Test_dataset =  <BatchDataset shapes: ((None, 19, 60, 60, 1), (None, 60, 60, 1)), types: (tf.float32, tf.float32)>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Procesar datos\n",
    "\n",
    "#Cargar datos (batch_size, time_step, pix, pix) image\n",
    "data_img_norm, data_vx_norm, data_vy_norm = load_normalize_data()\n",
    "\n",
    "pix_prueba = 60\n",
    "\n",
    "time_steps = data_img_norm.shape[1]\n",
    "height = pix_prueba\n",
    "width = pix_prueba\n",
    "channels = 1\n",
    "\n",
    "#Prueba simple\n",
    "data_img_norm = data_img_norm[:, :, 0:pix_prueba, 0:pix_prueba]\n",
    "data_vy_norm = data_vy_norm[:, 0:pix_prueba, 0:pix_prueba]\n",
    "\n",
    "print(\"Image Shape\", data_img_norm.shape)\n",
    "print(\"Image Max: \", data_img_norm.max(), \" Image Min: \", data_img_norm.min())\n",
    "\n",
    "print(\"Vx Shape\", data_vx_norm.shape)\n",
    "print(\"Vx Max: \", data_vx_norm.max(), \" Vx Min: \", data_vx_norm.min())\n",
    "\n",
    "print(\"Vy Shape\", data_vy_norm.shape)\n",
    "print(\"Vy Max: \", data_vy_norm.max(), \" Vy Min: \", data_vy_norm.min())\n",
    "\n",
    "\n",
    "# Adaptar los datos para PyTorch y generar conjuntos de entrenamiento\n",
    "train_images, valid_images, test_images, train_velocity, valid_velocity, test_velocity, batch_size = \\\n",
    "adapt_training_data_Keras(data_img_norm, data_vy_norm)\n",
    "\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset, valid_dataset, test_dataset = prepare_datasets(train_images, valid_images, test_images,\n",
    "                                                          train_velocity, valid_velocity, test_velocity,\n",
    "                                                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3c1c9-7d6b-4a27-9484-43354355647c",
   "metadata": {},
   "source": [
    "### MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e61e445-30ff-4499-ab43-3ac80022e37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_6 (ConvLSTM2D)  (None, 60, 60, 64)        150016    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 60, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 460800)            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               58982528  \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 59,206,529\n",
      "Trainable params: 59,206,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Definir una estrategia de distribución\n",
    "strategy = tf.distribute.MirroredStrategy(devices=['/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3'])\n",
    "\n",
    "class DistributedDeepSanneModel(tf.keras.Model):\n",
    "    def __init__(self, time_steps, height, width, channels):\n",
    "        super(DistributedDeepSanneModel, self).__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "        \n",
    "        # Modelo secuencial para las capas asignadas a las GPUs\n",
    "        self.model = tf.keras.Sequential([\n",
    "            # Capa ConvLSTM2D para asignar a la GPU 0\n",
    "            tf.keras.layers.ConvLSTM2D(\n",
    "                filters=64,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=(1, 1),\n",
    "                padding='same',\n",
    "                activation='relu',\n",
    "                input_shape=(self.time_steps, self.height, self.width, self.channels)\n",
    "            ),\n",
    "            \n",
    "            # Capa Conv2D para asignar a la GPU 1\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=128,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=(1, 1),\n",
    "                padding='same',\n",
    "                activation='relu'\n",
    "            ),\n",
    "            \n",
    "            # Capa Flatten para asignar a la GPU 2\n",
    "            tf.keras.layers.Flatten(),\n",
    "            \n",
    "            # Capa Dense para asignar a la GPU 3\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            \n",
    "            # Capa de salida para asignar a la GPU 0\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "# Instanciar modelo dentro de la estrategia de distribución\n",
    "with strategy.scope():\n",
    "    model = DistributedDeepSanneModel(time_steps=time_steps, height=height, width=width, channels=channels)\n",
    "\n",
    "# Compilar modelo \n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Mostrar la arquitectura del modelo\n",
    "model.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "addc5f2a-bc9a-4624-9e4f-e2d9766fb336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Model: \"distributed_deep_sanne_model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             multiple                  128       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_10 (ConvLSTM2D) multiple                  1573376   \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             multiple                  16512     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             multiple                  129       \n",
      "=================================================================\n",
      "Total params: 1,590,145\n",
      "Trainable params: 1,590,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Distribucion manual y controlada de tensores en dispositivos.\n",
    "\n",
    "# Definir una estrategia de distribución\n",
    "strategy = tf.distribute.MirroredStrategy(devices=['/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3'])\n",
    "\n",
    "class DistributedDeepSanneModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DistributedDeepSanneModel, self).__init__()\n",
    "        \n",
    "        # Asignar la primera capa a la GPU 0\n",
    "        with tf.device('/gpu:0'):\n",
    "            self.dense1 = tf.keras.layers.Dense(64, activation='linear', input_shape=(time_steps, height, width, channels))\n",
    "        \n",
    "        # Asignar la segunda capa a la GPU 1\n",
    "        with tf.device('/gpu:1'):\n",
    "                self.convLSTM = tf.keras.layers.ConvLSTM2D(\n",
    "                filters=128,  # Canales de salida 128\n",
    "                kernel_size=(4, 4),  # tamaño del kernel 4x4\n",
    "                strides=(1, 1),  # por defecto\n",
    "                padding='same',  # mantener dimensionalidad entrada salida\n",
    "                activation='relu',  # función de activacion ReLU\n",
    "                return_sequences=False,  # retorna solo la última salida en la secuencia de salidas\n",
    "                recurrent_activation='hard_sigmoid' \n",
    "            )\n",
    "        \n",
    "        # Asignar la tercera capa a la GPU 2\n",
    "        with tf.device('/gpu:2'):\n",
    "            self.dense2 = tf.keras.layers.Dense(128, activation='linear')\n",
    "        \n",
    "        # Asignar la cuarta capa a la GPU 3\n",
    "        with tf.device('/gpu:3'):\n",
    "            self.dense3 = tf.keras.layers.Dense(1, activation='linear')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.convLSTM(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "# Crear una instancia del modelo dentro del alcance de la estrategia\n",
    "with strategy.scope():\n",
    "    modelTF = DistributedDeepSanneModel()\n",
    "    modelTF.build((None, time_steps, height, width, channels))  # Construir el modelo con una entrada dummy\n",
    "    \n",
    "    # Compilar el modelo (se puede personalizar según tus necesidades)\n",
    "    modelTF.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Mostrar la arquitectura del modelo\n",
    "modelTF.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf48fb4-cfad-4a15-9bbf-35fd668a9648",
   "metadata": {},
   "source": [
    "### ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "892f90e8-04cc-447f-8b44-f4772753eb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer sequential_9 output shape: (45, 1)\n"
     ]
    }
   ],
   "source": [
    "# Dentro del ciclo de entrenamiento, después de cada batch\n",
    "for inputs, targets in train_dataset:\n",
    "    # Hacer una predicción con el modelo\n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    # Imprimir las formas de las salidas de las capas intermedias\n",
    "    for layer in model.layers:\n",
    "        intermediate_output = layer(inputs)\n",
    "        print(f\"Layer {layer.name} output shape: {intermediate_output.shape}\")\n",
    "    \n",
    "    # Continuar con el entrenamiento normalmente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "937b6cd2-e10c-4d97-ac83-276ea1de8160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 08:06:35.293974: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 19\n",
      "        }\n",
      "        dim {\n",
      "          size: 60\n",
      "        }\n",
      "        dim {\n",
      "          size: 60\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 60\n",
      "        }\n",
      "        dim {\n",
      "          size: 60\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_9 (ConvLSTM2D)  (None, 60, 60, 64)        150016    \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 60, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 460800)            0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               58982528  \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 59,206,529\n",
      "Trainable params: 59,206,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training:   0%|                                                                              | 0/10 [00:00<?, ?epoch/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                              | 0/10 [01:40<?, ?epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 9 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 9 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "5 root error(s) found.\n  (0) Invalid argument:  Incompatible shapes: [9,60,60,1] vs. [9,1]\n\t [[node replica_3/mean_squared_error/SquaredDifference (defined at disk2/alma/anaconda3/envs/tf-gpu/lib/python3.9/threading.py:980) ]]\n\t [[div_no_nan/ReadVariableOp_3/_680]]\n  (1) Invalid argument:  Incompatible shapes: [9,60,60,1] vs. [9,1]\n\t [[node replica_3/mean_squared_error/SquaredDifference (defined at disk2/alma/anaconda3/envs/tf-gpu/lib/python3.9/threading.py:980) ]]\n  (2) Invalid argument:  Incompatible shapes: [9,60,60,1] vs. [9,1]\n\t [[node replica_3/mean_squared_error/SquaredDifference (defined at disk2/alma/anaconda3/envs/tf-gpu/lib/python3.9/threading.py:980) ]]\n\t [[div_no_nan/_699]]\n  (3) Invalid argument:  Incompatible shapes: [9,60,60,1] vs. [9,1]\n\t [[node replica_3/mean_squared_error/SquaredDifference (defined at disk2/alma/anaconda3/envs/tf-gpu/lib/python3.9/threading.py:980) ]]\n\t [[Adam/Adam/group_deps/NoOp_1/_747]]\n  (4) Invalid argument:  Incompatible shapes: [9,60,60,1] vs. [9,1]\n\t [[node replica_3/mean_squared_error/SquaredDifference (defined at disk2/alma/anaconda3/envs/tf-gpu/lib/python3.9/threading.py:980) ]]\n\t [[replica_1/distributed_deep_sanne_model_10/sequential_9/conv_lst_m2d_9/while/body/_789/replica_1/distributed_deep_sanne_model_10/sequential_9/conv_lst_m2d_9/while/gradient_tape/replica_1/distributed_deep_sanne_model_10/sequential_9/conv_lst_m2d_9/while/gradients/replica_1/distributed_deep_sanne_model_10/sequential_9/conv_lst_m2d_9/while/mul_5_grad/Shape-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer/_599]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_50367]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node replica_3/mean_squared_error/SquaredDifference:\n replica_3/distributed_deep_sanne_model_10/sequential_9/dense_19/BiasAdd (defined at tmp/ipykernel_1310266/2817332169.py:46)\t\n cond_7/Identity_1 (defined at tmp/ipykernel_1310266/3690703949.py:32)\n\nInput Source operations connected to node replica_3/mean_squared_error/SquaredDifference:\n replica_3/distributed_deep_sanne_model_10/sequential_9/dense_19/BiasAdd (defined at tmp/ipykernel_1310266/2817332169.py:46)\t\n cond_7/Identity_1 (defined at tmp/ipykernel_1310266/3690703949.py:32)\n\nInput Source operations connected to node replica_3/mean_squared_error/SquaredDifference:\n replica_3/distributed_deep_sanne_model_10/sequential_9/dense_19/BiasAdd (defined at tmp/ipykernel_1310266/2817332169.py:46)\t\n cond_7/Identity_1 (defined at tmp/ipykernel_1310266/3690703949.py:32)\n\nInput Source operations connected to node replica_3/mean_squared_error/SquaredDifference:\n replica_3/distributed_deep_sanne_model_10/sequential_9/dense_19/BiasAdd (defined at tmp/ipykernel_1310266/2817332169.py:46)\t\n cond_7/Identity_1 (defined at tmp/ipykernel_1310266/3690703949.py:32)\n\nInput Source operations connected to node replica_3/mean_squared_error/SquaredDifference:\n replica_3/distributed_deep_sanne_model_10/sequential_9/dense_19/BiasAdd (defined at tmp/ipykernel_1310266/2817332169.py:46)\t\n cond_7/Identity_1 (defined at tmp/ipykernel_1310266/3690703949.py:32)\n\nFunction call stack:\ntrain_function -> train_function -> train_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[1;32m     31\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 32\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Evaluar en el conjunto de prueba\u001b[39;00m\n\u001b[1;32m     35\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:888\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    885\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[0;32m--> 888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m   _, _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    891\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    892\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[1;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m     args,\n\u001b[1;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1923\u001b[0m     executing_eagerly)\n\u001b[1;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 5 root error(s) found.\n  (0) Invalid argument:  Incompatible shapes: [9,60,60,1] vs. [9,1]\n\t [[node replica_3/mean_squared_error/SquaredDifference (defined at disk2/alma/anaconda3/envs/tf-gpu/lib/python3.9/threading.py:980) ]]\n\t [[div_no_nan/ReadVariableOp_3/_680]]\n  (1) Invalid argument:  Incompatible shapes: [9,60,60,1] vs. [9,1]\n\t [[node replica_3/mean_squared_error/SquaredDifference (defined at disk2/alma/anaconda3/envs/tf-gpu/lib/python3.9/threading.py:980) ]]\n  (2) Invalid argument:  Incompatible shapes: [9,60,60,1] vs. [9,1]\n\t [[node replica_3/mean_squared_error/SquaredDifference (defined at disk2/alma/anaconda3/envs/tf-gpu/lib/python3.9/threading.py:980) ]]\n\t [[div_no_nan/_699]]\n  (3) Invalid argument:  Incompatible shapes: [9,60,60,1] vs. [9,1]\n\t [[node replica_3/mean_squared_error/SquaredDifference (defined at disk2/alma/anaconda3/envs/tf-gpu/lib/python3.9/threading.py:980) ]]\n\t [[Adam/Adam/group_deps/NoOp_1/_747]]\n  (4) Invalid argument:  Incompatible shapes: [9,60,60,1] vs. [9,1]\n\t [[node replica_3/mean_squared_error/SquaredDifference (defined at disk2/alma/anaconda3/envs/tf-gpu/lib/python3.9/threading.py:980) ]]\n\t [[replica_1/distributed_deep_sanne_model_10/sequential_9/conv_lst_m2d_9/while/body/_789/replica_1/distributed_deep_sanne_model_10/sequential_9/conv_lst_m2d_9/while/gradient_tape/replica_1/distributed_deep_sanne_model_10/sequential_9/conv_lst_m2d_9/while/gradients/replica_1/distributed_deep_sanne_model_10/sequential_9/conv_lst_m2d_9/while/mul_5_grad/Shape-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer/_599]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_50367]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node replica_3/mean_squared_error/SquaredDifference:\n replica_3/distributed_deep_sanne_model_10/sequential_9/dense_19/BiasAdd (defined at tmp/ipykernel_1310266/2817332169.py:46)\t\n cond_7/Identity_1 (defined at tmp/ipykernel_1310266/3690703949.py:32)\n\nInput Source operations connected to node replica_3/mean_squared_error/SquaredDifference:\n replica_3/distributed_deep_sanne_model_10/sequential_9/dense_19/BiasAdd (defined at tmp/ipykernel_1310266/2817332169.py:46)\t\n cond_7/Identity_1 (defined at tmp/ipykernel_1310266/3690703949.py:32)\n\nInput Source operations connected to node replica_3/mean_squared_error/SquaredDifference:\n replica_3/distributed_deep_sanne_model_10/sequential_9/dense_19/BiasAdd (defined at tmp/ipykernel_1310266/2817332169.py:46)\t\n cond_7/Identity_1 (defined at tmp/ipykernel_1310266/3690703949.py:32)\n\nInput Source operations connected to node replica_3/mean_squared_error/SquaredDifference:\n replica_3/distributed_deep_sanne_model_10/sequential_9/dense_19/BiasAdd (defined at tmp/ipykernel_1310266/2817332169.py:46)\t\n cond_7/Identity_1 (defined at tmp/ipykernel_1310266/3690703949.py:32)\n\nInput Source operations connected to node replica_3/mean_squared_error/SquaredDifference:\n replica_3/distributed_deep_sanne_model_10/sequential_9/dense_19/BiasAdd (defined at tmp/ipykernel_1310266/2817332169.py:46)\t\n cond_7/Identity_1 (defined at tmp/ipykernel_1310266/3690703949.py:32)\n\nFunction call stack:\ntrain_function -> train_function -> train_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clase de callback para mostrar progreso con tqdm\n",
    "class TQDMCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "        self.tqdm.update(1)\n",
    "        self.tqdm.set_postfix(logs)\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = 0\n",
    "        self.tqdm = tqdm(total=self.params['epochs'], desc=\"Training\", unit=\"epoch\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.tqdm.close()\n",
    "\n",
    "\n",
    "# Definir una estrategia de distribución\n",
    "strategy = tf.distribute.MirroredStrategy(devices=['/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3'])\n",
    "\n",
    "# Crear y compilar el modelo dentro de la estrategia\n",
    "with strategy.scope():\n",
    "    model = DistributedDeepSanneModel(time_steps=time_steps, height=height, width=width, channels=channels)\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss=MeanSquaredError())\n",
    "\n",
    "    # Mostrar el resumen del modelo\n",
    "    model.model.summary()\n",
    "\n",
    "    # Definir callbacks, como el de progreso con tqdm\n",
    "    callbacks = [TQDMCallback()]\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    epochs = 10\n",
    "    history = model.fit(train_dataset, epochs=epochs, validation_data=valid_dataset, callbacks=callbacks)\n",
    "\n",
    "    # Evaluar en el conjunto de prueba\n",
    "    test_loss = model.evaluate(test_dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Guardar los resúmenes de entrenamiento, validación y prueba\n",
    "    train_summaries = history.history['loss']\n",
    "    valid_summaries = history.history['val_loss']\n",
    "\n",
    "    with open('train_summaries.txt', 'w') as f:\n",
    "        for epoch, loss in enumerate(train_summaries, 1):\n",
    "            f.write(f\"Epoch {epoch}, Training Loss: {loss:.4f}\\n\")\n",
    "\n",
    "    with open('valid_summaries.txt', 'w') as f:\n",
    "        for epoch, loss in enumerate(valid_summaries, 1):\n",
    "            f.write(f\"Epoch {epoch}, Validation Loss: {loss:.4f}\\n\")\n",
    "\n",
    "    with open('test_summaries.txt', 'w') as f:\n",
    "        f.write(f\"Test Loss: {test_loss:.4f}\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
