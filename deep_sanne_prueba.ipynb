{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR5aoENgON4LFxYOHrvPEZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALTH-Alma/Deep-model-test/blob/main/deep_sanne_prueba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arquitectura Deep-SANNE**\n",
        "\n",
        "**Input Layer:** La capa de entrada recibe secuencias con un total de 24 pasos temporales, cada uno con dimensiones de 30x30.\n",
        "\n",
        "**Primera Capa Densamente Conectada:** Tras la capa de entrada, hay una capa densamente conectada (fully-connected) con activación lineal, que aumenta el número de canales a 64, manteniendo los 24 pasos temporales y las dimensiones espaciales de 30x30.\n",
        "\n",
        "**Convolutional LSTM Layer:** Esta capa consiste en 24 celdas LSTM que realizan operaciones convolucionales, aumentando el número de canales a 128. El kernel convolucional utilizado es de 2D con un tamaño de 4x4, moviéndose a través de las dimensiones espaciales (n, m). La función de activación utilizada en esta capa es la unidad lineal rectificada (ReLU).\n",
        "\n",
        "**Segunda Capa Densamente Conectada:** Solo la salida de la última celda LSTM se considera y se alimenta a otra capa densamente conectada con activación lineal y las mismas dimensiones aumentadas de 30x30x128.\n",
        "\n",
        "**Capa Densamente Conectada Final:** Finalmente, hay una última capa densamente conectada que reduce las dimensiones de nuevo a 30x30, sirviendo como la salida de la red."
      ],
      "metadata": {
        "id": "mfNfhTPWnySi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prueba\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WMxKyI0ZbmlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get dataset de prueba ---- Probar cuanto aguanta de memoria\n",
        "'''(train_images, train_vectorial), (test_images, test_vectorial) = some..\n",
        " plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(train_vectorial[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show() '''\n",
        "\n",
        "#  probar cuanto cae en memoria de un dispositivo\n",
        "#Construir modelo: multidispositivos\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    #Imput layer t*(n_1*m_1)\n",
        "    tf.keras.Input(\n",
        "        shape=(24, 30, 30, 1) #Considerando entrada de 24 pasos de tiempo, todas de 30 x 30, canales color (3)\n",
        "    ),\n",
        "    #First Densely connected layer t*(n_2*m_2*l_2) / l_2=64  (configuracion de un imput por canal - averiguar)\n",
        "    tf.keras.layers.Dense(\n",
        "        units = 64, # aumenta el numero de canales a 64\n",
        "        activation='linear' # función de activacion lineal\n",
        "    ),\n",
        "    #2D Convolutional LSTM layer\n",
        "    tf.keras.layers.ConvLSTM2D(\n",
        "        filters=128,  # Canaless de salida 128\n",
        "        kernel_size=(4, 4),  # tamaño del kernel 4x4\n",
        "        strides=(1, 1),  # por defecto\n",
        "        padding='same',  # mantener dimensionalidad entrada salida\n",
        "        activation='relu',  # función de activacion ReLU\n",
        "        return_sequences=False,  # retorna secuencias para permitir conexiones secuenciales  (devuelve solo la ultima salida en la secuencia de salidas)\n",
        "        recurrent_activation='hard_sigmoid',  # 'hard_sigmoid' es una opción común para recurrent_activation\n",
        "    ),\n",
        "    #Second Densely connected layer (n_4*m_4*l_4) / l_4=128  Only the output of the last LSTM cell is considered return_sequences=False cumple\n",
        "    tf.keras.layers.Dense(\n",
        "        units = 128, # aumenta el numero de canales a 128\n",
        "        activation='linear' # función de activacion lineal\n",
        "    ),\n",
        "    #Finally Densely Connected Layer (Output Layer)\n",
        "    tf.keras.layers.Dense(units=1, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "])"
      ],
      "metadata": {
        "id": "3kt8gbL4btyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Primera capa densamente conectada:**\n",
        "\n",
        "Aumenta el número de canales a 64.\n",
        "\n",
        "Mantiene los 24 pasos temporales y las dimensiones espaciales de 30x30.\n",
        "\n",
        "Utiliza una función de activación lineal."
      ],
      "metadata": {
        "id": "c7OfHsA813hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capa de entrada:**\n",
        "\n",
        "Recibe secuencias de 24 pasos temporales.\n",
        "\n",
        "Cada paso tiene dimensiones espaciales de 30x30.\n",
        "\n",
        "Posee 3 canales para el color."
      ],
      "metadata": {
        "id": "MV7Geix81yJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capa ConvLSTM2D:**\n",
        "\n",
        "Contiene 24 celdas LSTM que realizan operaciones convolucionales.\n",
        "\n",
        "Aumenta el número de canales a 128.\n",
        "\n",
        "Utiliza un kernel convolucional 2D de tamaño 4x4.\n",
        "\n",
        "Se mueve a través de las dimensiones espaciales (n, m).\n",
        "\n",
        "Posee una función de activación ReLU.\n",
        "\n",
        "Retorno de secuencias: Falso, solo se considera la última salida de la secuencia.\n",
        "\n",
        "Activación recurrente: 'hard_sigmoid', una opción común para redes LSTM."
      ],
      "metadata": {
        "id": "R5c9lIQu1-tW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Segunda capa densamente conectada:**\n",
        "\n",
        "Solo se utiliza la salida de la última celda LSTM.\n",
        "\n",
        "Aumenta las dimensiones a 30x30x128.\n",
        "\n",
        "Posee una función de activación lineal."
      ],
      "metadata": {
        "id": "GHmaorkb1_AW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capa final:**\n",
        "\n",
        "Reduce las dimensiones a 30x30.\n",
        "\n",
        "Genera la salida final de la red.\n",
        "\n",
        "**Función de activación tanh (preguntar):\n",
        "Rango de salida: [-1, 1].\n",
        "Preserva la magnitud y dirección de los campos vectoriales.\n",
        "Adecuada para la representación de campos vectoriales con valores positivos y negativos."
      ],
      "metadata": {
        "id": "OpyPMtZ32XSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import helper\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib as plt"
      ],
      "metadata": {
        "id": "9d3oFzYAcewe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1=nn.Flatten()\n",
        "l1=nn.Linear(28*28, 300)\n",
        "r1=nn.ReLU()\n",
        "l2=nn.Linear(300, 100)\n",
        "r2=nn.ReLU()\n",
        "l3=nn.Linear(100, 30)\n",
        "s3=nn.Softmax()\n",
        "model = nn.Sequential(\\\n",
        "    f1,\n",
        "    l1,\n",
        "    r1,\n",
        "    l2,\n",
        "    r2,\n",
        "    l3,\n",
        "    s3, \\\n",
        ")\n",
        "\n",
        "print(\"Model: \", model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zEIc4PeduZB",
        "outputId": "8edd80b7-010b-4d34-c973-c6a00ca14f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Linear(in_features=784, out_features=300, bias=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=100, out_features=30, bias=True)\n",
            "  (6): Softmax(dim=None)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n"
      ],
      "metadata": {
        "id": "au9N0QleeI0t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First Densely Layer\n",
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, units, activation):\n",
        "        super(DenseLayer, self).__init__()\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "\n",
        "        self.linear = nn.Linear(1, units)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # dimensiones de entrada --averiguar\n",
        "        #x = x.view(-1, 1) # considerando una dimension\n",
        "        x = self.linear(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "dense_layer_1 = DenseLayer(units=64, activation=F.linear)\n"
      ],
      "metadata": {
        "id": "tEO0Xm7WejON"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional LTSM Layer\n",
        "\n",
        "# Definir celda LSTM convolucional\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, filters, kernel_size):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Puertas de entrada y olvido\n",
        "        self.conv_gate = nn.Conv2d(filters * 4, filters * 4, kernel_size=kernel_size, padding=1)\n",
        "        # Puerta de salida\n",
        "        self.conv_out = nn.Conv2d(filters * 2, filters, kernel_size=kernel_size, padding=1)\n",
        "\n",
        "    def forward(self, x, h_cell, c_cell):\n",
        "        # Concatenar datos de entrada con estado oculto anterior h_cell\n",
        "        concat = torch.cat([x, h_cell], dim=1)\n",
        "\n",
        "        # Aplicar la capa de convolucion\n",
        "        gates = self.conv_gate(concat)\n",
        "\n",
        "        # Divider las dalidas de la convolucion en las puertas de entrada, olvido, salida y puerta de cekda\n",
        "        i, f, o, g = torch.split(gates, self.filters, dim=1)\n",
        "        # Activacion sig\n",
        "        i = F.sigmoid(i)\n",
        "        f = F.sigmoid(f)\n",
        "        o = F.sigmoid(o)\n",
        "        # Activacion tanh para puerta de celda\n",
        "        g = F.tanh(g)\n",
        "\n",
        "        # Actualizar estado de la celda\n",
        "        c_new = f * c_cell + i * g\n",
        "\n",
        "        # Actualizar salida. Funcion de activacion tanh a la concatenacion de datos de entrada y nuevos estados... cambiar dim\n",
        "        out = o * F.tanh(self.conv_out(torch.cat([x, c_new], dim=1)))\n",
        "\n",
        "        # Returnar salida y estado actual\n",
        "        return out, c_new\n",
        "\n",
        "\n",
        "# Definir capa LSTM convolucional en 2D\n",
        "class ConvLSTM2D(nn.Module):\n",
        "    def __init__(self, filters, kernel_size, strides=(1, 1), padding='same', activation='relu'):\n",
        "        super(ConvLSTM2D, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.activation = activation\n",
        "\n",
        "        self.conv_lstm_cell = ConvLSTMCell(filters, kernel_size) # crear instancia de celda LSTM para realizar la operacion LSTM convolucional\n",
        "\n",
        "    def forward(self, x, h_state=None):\n",
        "        batch_size, seq_len, in_channels, height, width = x.size()\n",
        "\n",
        "        # Inicializar estado oculto y celda en caso de no entregarse\n",
        "        if h_state is None:\n",
        "            h_state = (torch.zeros(batch_size, self.filters, height, width, device=x.device),\n",
        "                       torch.zeros(batch_size, self.filters, height, width, device=x.device))\n",
        "\n",
        "        hidden_states = []\n",
        "        cell_states = []\n",
        "\n",
        "        # Iterar sobre los elementos de la secuencia de entreda\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :, :]\n",
        "            h_t, c_t = self.conv_lstm_cell(x_t, *h_state) #Aplicaar celda convolucionar a cada elemento\n",
        "            hidden_states.append(h_t)\n",
        "            cell_states.append(c_t)\n",
        "            h_state = (h_t, c_t)  # Actualizar estado\n",
        "\n",
        "        hidden_states = torch.stack(hidden_states, dim=1)  # Almacenar estados ocultos\n",
        "        cell_states = torch.stack(cell_states, dim=1)  # Almacenar estados de celda\n",
        "\n",
        "        # Solo retorna el ultimo resultado (return_sequences=False)\n",
        "        return hidden_states[:, -1, :, :], cell_states[:, -1, :, :]\n",
        "\n",
        "conv_lstm2d_layer = ConvLSTM2D(\n",
        "    filters=128,\n",
        "    kernel_size=(4, 4),\n",
        "    strides=(1, 1),\n",
        "    padding='same',\n",
        "    activation=F.relu,\n",
        ")\n"
      ],
      "metadata": {
        "id": "CHMn_iMNetH9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Second Densely Layer\n",
        "class DenseLayer2(nn.Module):\n",
        "    def __init__(self, units, activation):\n",
        "        super(DenseLayer2, self).__init__()\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "\n",
        "        self.linear = nn.Linear(128, units)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "dense_layer_2 = DenseLayer2(units=128, activation=F.linear)\n"
      ],
      "metadata": {
        "id": "xcoMuQ8ve8b8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Output Layer\n",
        "class OutputLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OutputLayer, self).__init__()\n",
        "        self.linear = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x\n",
        "\n",
        "output_layer = OutputLayer()\n"
      ],
      "metadata": {
        "id": "443d7lyhfFXl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Create\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.dense_layer_1 = dense_layer_1\n",
        "        self.conv_lstm2d_layer = conv_lstm2d_layer\n",
        "        self.dense_layer_2 = dense_layer_2\n",
        "        self.output_layer = output_layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense_layer_1(x)\n",
        "        x = self.conv_lstm2d_layer(x)\n",
        "        x = self.dense_layer_2(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "model = Model()\n"
      ],
      "metadata": {
        "id": "0LOD5j6wfRtF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "MCRKCjPsg3Er",
        "outputId": "fb76f3e7-312e-4c20-8010-ce0ea729abf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (dense_layer_1): DenseLayer(\n",
            "    (linear): Linear(in_features=1, out_features=64, bias=True)\n",
            "  )\n",
            "  (conv_lstm2d_layer): ConvLSTM2D(\n",
            "    (conv_lstm_cell): ConvLSTMCell(\n",
            "      (conv_gate): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "      (conv_out): Conv2d(256, 128, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (dense_layer_2): DenseLayer2(\n",
            "    (linear): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (output_layer): OutputLayer(\n",
            "    (linear): Linear(in_features=128, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}